# -*- coding: utf-8 -*-
"""project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18RFqiJBlBDSzXyxxgKL0FRJ0jdxpd9Eu
"""

from collections import Counter
import csv
import os
import re
import time

import matplotlib.pyplot as plt

def loadfile(file):
  ''' loads a file and reads lines,
      separating labels and text 
      '''
  if not os.path.isfile(file):
    print("File not found")
    exit(-1)

  X = []
  y = [] 

  with open(file, "r") as train_f:
    train_f.readline()
    lines = csv.reader(train_f)
    for line in lines:
      y.append(line[0])
      X.append(line[1])
  
  return X, y

def shrink(data, p):
  if p > 1.0:
    print("Error: Proportion must be less than 1")
    exit(-1)
  return data[:int(len(data) * p)]

def flatten_list_of_text(text):
  ''' converts list of strings into single string '''
  return " ".join(text)

def clean_text(text):
  ''' removes non-alphanum chars from str and converts to lowercase '''
  return re.sub(r'[\W_]+', ' ', text.lower())

def remove_stopwords(text):
  ''' removes instances of words in stopwords.txt from list '''
  if not os.path.isfile("stopwords.txt"):
    print("Error: stopwords.txt not in directory")
    exit(-1)

  if type(text) is not list:
    text = text.split()

  with open("stopwords.txt", "r") as stopwords:
    for word in stopwords:
      word = re.sub(r'\s', '', word)
      text = list(filter((word).__ne__, text))
  
  return text

def get_most_frequent_words(data, N):
  ''' returns the N most frequent words in a list of words '''
  return Counter(data).most_common(N)

def calc_likelihoods(X, y, classes, L, laplace=True):
  ''' calculates the likelihoods of each feature in L for each class'''
  # get cleaned data for each class
  X_div = []
  for c in classes:
    X_div.append(flatten_list_of_text(
        [clean_text(X[i]) for i in range(len(X)) if y[i] == c]).split())
    
  # get probabilities for each feature based on class
  for word in L.keys():
    L[word] = []
    for c in X_div:
      if laplace:
        prob = (c.count(word) + 1) / (len(c) + len(L))
      else:
        prob = c.count(word) / len(c)
      L[word].append(prob)

  return X_div

def calc_priors(y, classes):
  ''' calculates the priors for each class'''
  priors = [0.0] * len(classes)
  for i, c in enumerate(classes):
    priors[i] = y.count(c) / len(y)
  return priors

class NBModel:
  def __init__(self, classes=[], L={}, priors=[]):
    self.classes = classes
    self.L = L
    self.priors = priors
  
  def train(self, X, y, features, laplace=True):
    ''' calculates the likelihoods and priors of train data '''
    self.classes = list(set(y))
    self.L = dict(features)
    self.priors = calc_priors(y, self.classes)
    calc_likelihoods(X, y, self.classes, self.L, laplace=laplace)

  def test(self, data):
    ''' returns most likely class of review or list of reviews '''
    def test_single_review(review):
      proba = self.priors.copy()
      for word in clean_text(review).split():
        if prob := self.L.get(word):
          proba = list(map(lambda x,y: x*y, proba, prob))
      return self.classes[proba.index(sorted(proba, reverse=True)[0])]

    if type(data) is str:
      return test_single_review(data)
    elif type(data) is list:
      return list(map(test_single_review, data))
    else:
      print("Error: data formatted incorrectly")
      exit(-1)

def acc(pred, y):
  ''' gets accuracy of predictions compared to y (labels) '''
  n = len(pred)
  acc = 0.0
  if n == len(y) and n > 0:
     acc = sum([1 for i in range(n) if pred[i] == y[i]]) / n
  else:
    print("Error: list length mismatch")
    exit(-1)
  return acc

if __name__ == '__main__':
  print("Loading files...")
  X, y = loadfile("train.csv")
  test_X, test_y = loadfile("test.csv")
  print("Files loaded successfully.")

  N = 1000
  print("Extracting features (this may take a minute)...")
  features = get_most_frequent_words(remove_stopwords(clean_text(flatten_list_of_text(X))), N)
  print("Top 50 features: ", *features[:50], sep="\n")

  model = NBModel()

  sizes = [0.1, 0.3, 0.5, 0.7, 1.0]
  history = []

  print("\nBeginning model training")
  for size in sizes:
    print(f"Number of data points: {int(len(X)*size)} ({size*100}%)")
    print("Training...")
    start = time.time()
    model.train(shrink(X, size), shrink(y, size), features)
    end = time.time()
    print(f"Training complete. Time elapsed: {end - start}s")
    pred = model.test(test_X)
    accuracy = acc(pred, test_y)
    print("Accuracy: ", accuracy)
    history.append(accuracy)

  print("Displaying results...")
  plt.plot(sizes, history)
  plt.title('Results')
  plt.xlabel('Percentage of training data used')
  plt.ylabel('Accuracy (%)')
  plt.show()

  tp = sum([1 for i in range(len(pred)) if pred[i] == test_y[i] and pred[i] == '5'])
  fp = sum([1 for i in range(len(pred)) if pred[i] != test_y[i] and pred[i] == '5'])
  tn = sum([1 for i in range(len(pred)) if pred[i] == test_y[i] and pred[i] == '1'])
  fn = sum([1 for i in range(len(pred)) if pred[i] != test_y[i] and pred[i] == '1'])
  print(f"Confusion matrix\n{tp} | {fp}\n{fn}  | {tn}")

  print("Exit")